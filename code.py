# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AViZQUrqY-odWF3UE02FIYxn-22aqR-A

Installing Transformer
"""

!pip install transformers

"""Import drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import os
import pandas as pd
import os
import pandas as pd
import numpy as np
#visualizations
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#consistent sized plot 
from pylab import rcParams
rcParams['figure.figsize']=12,5
rcParams['axes.labelsize']=12
rcParams['xtick.labelsize']=12
rcParams['ytick.labelsize']=12

#handle the warnings in the code
import warnings
warnings.filterwarnings(action='ignore',category=DeprecationWarning)
warnings.filterwarnings(action='ignore',category=FutureWarning)

#text preprocessing libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.tokenize import WordPunctTokenizer
from nltk.tokenize import TweetTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import re
pd.options.display.max_columns = None

from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix,classification_report
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import scale,StandardScaler
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer,CountVectorizer
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.preprocessing.text import Tokenizer
from keras.callbacks import EarlyStopping
from keras.preprocessing import sequence
from keras.optimizers import RMSprop
from keras.models import Model
import tensorflow as tf
import torch

from warnings import filterwarnings
filterwarnings("ignore")

student_id = 2201829 # Note this is an interger and you need to input your id

"""Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"""

# set same seeds for all libraries

#numpy seed
np.random.seed(student_id)
torch.manual_seed(student_id)

"""Mount Google Drive"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id)) # Make sure to update with your student_id and student_id is an integer
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('Train file: ', train_file)

"""Creating all directories directories"""

MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1')              # Model 1

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25')

MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50')

MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75')

MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100')

#---------------------------------------------------------------#               # Model 2

MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2')
print('Model 2 directory: ', MODEL_2_DIRECTORY)

MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25')

MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50')

MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75')

MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100')

"""Output Directories"""

#Model 1

model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') 

model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') 

model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv')

model_1_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv')

#------------------------------------------------------------------------#
#Model 2
model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv')

model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') 

model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') 

model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv')

"""Creating data file different size"""

import pandas as pd

# Shuffle the dataset (optional)
data = pd.read_csv(train_file) 
dataset = data.sample(frac=1, random_state=42).reset_index(drop=True)

# Create separate DataFrames for each class
df_not = dataset[dataset['label'] == 'NOT']
df_off = dataset[dataset['label'] == 'OFF']

# Calculate the number of samples to take from each class for 25%, 50% anf 75% split

split_size_25_off = int(len(df_off) * 0.3761)
split_size_25_not = int(len(df_not) * 0.1873)

split_size_50_off = int(len(df_off) * 0.752)
split_size_50_not = int(len(df_not) * 0.3746)

split_size_75_off = int(len(df_off))
split_size_75_not = int(len(df_not) * 0.6256)

# Take the first split_size samples from each class
df_25_class1 = df_not.iloc[:split_size_25_not]
df_25_class2 = df_off.iloc[:split_size_25_off]
df_50_class1 = df_not.iloc[:split_size_50_not]
df_50_class2 = df_off.iloc[:split_size_50_off]
df_75_class1 = df_not.iloc[:split_size_75_not]
df_75_class2 = df_off.iloc[:split_size_75_off]

# Concatenate the subsets for each class vertically
df_25 = pd.concat([df_25_class1, df_25_class2], ignore_index=True)
df_50 = pd.concat([df_50_class1, df_50_class2], ignore_index=True)
df_75 = pd.concat([df_75_class1, df_75_class2], ignore_index=True)

# Shuffle the combined dataset
df_25 = df_25.sample(frac=1, random_state=42).reset_index(drop=True)
df_50 = df_50.sample(frac=1, random_state=42).reset_index(drop=True)
df_75 = df_75.sample(frac=1, random_state=42).reset_index(drop=True)

# Reset the index of the extracted portions
train_25 = df_25.reset_index(drop=True)
train_50 = df_50.reset_index(drop=True)
train_75 = df_75.reset_index(drop=True)

# Specify the output directory
output_directory = 'gdrive/MyDrive/./CE807/Assignment2/2201829/'

# Create the output directory if it doesn't exist
if not os.path.exists(output_directory):
    os.makedirs(output_directory)

# Save the divided datasets as separate CSV files in the output directory
train_25.to_csv(os.path.join(output_directory, 'train_25.csv'), index=False)
train_50.to_csv(os.path.join(output_directory, 'train_50.csv'), index=False)
train_75.to_csv(os.path.join(output_directory, 'train_75.csv'), index=False)

train_25 = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv')
train_50 = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv')
train_75 = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')

"""## Exploratory Data Analysis"""

df = pd.read_csv(train_file)
val_df = pd.read_csv(val_file)
test_df = pd.read_csv(test_file)
df.head()

"""Checking the shape of the train, valid and test dataset"""

df.shape, val_df.shape, test_df.shape

"""Check the data types of the columns"""

print(df.dtypes)
print(val_df.dtypes)
print(test_df.dtypes)

"""Check for missing values"""

print("Missing values: ")
print("#---------Train data-------------#")
print(df.isnull().sum())
print("#---------Validation data------------#")
print(val_df.isnull().sum())
print("#----------Test data-----------------#")
print(test_df.isnull().sum())

"""Ploting pie chart to find the percentage of number of 'NOT' and 'OFF' classes in train, valid and test dataset."""

def pie_chart(df):
  df['Count'] = df['label'].map(df['label'].value_counts())
  df['Percentage'] = (df['Count'] / df['Count'].sum()) * 100
  sizes = df.groupby('label')['Percentage'].first().values 
  return sizes

textprops = {'fontsize': 6}
fig, axes = plt.subplots(1, 3, figsize=(10, 5))
ax1 = axes[0]
ax2 = axes[1]
ax3 = axes[2]
label1 = df['label'].unique() 
label2 = df['label'].unique() 
label3 = df['label'].unique() 
size1 = pie_chart(df)
size2 = pie_chart(val_df)
size3 = pie_chart(test_df)
ax1.pie(size1, labels= label1, autopct='%1.1f%%', startangle=90, textprops=textprops)
ax1.axis('equal')
ax1.set_title( 'Train data ')
ax2.pie(size2, labels= label2, autopct='%1.1f%%', startangle=90, textprops=textprops)
ax2.axis('equal')
ax2.set_title( 'Validation data')
ax3.pie(size3, labels= label3, autopct='%1.1f%%', startangle=90, textprops=textprops)
ax3.axis('equal')
ax3.set_title( 'Test data')

"""Word frequency, which can be used to remove additional stop words."""

input_text_words = ' '.join(df['tweet']).split()
input_text_word_freq = pd.Series(input_text_words).value_counts()
input_text_word_freq[:50].plot(kind='bar')
plt.title('Top 20 most common words in input text')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.show()

"""This can be also used for finding most frequent words"""

from wordcloud import WordCloud
wordcloud_input_text = WordCloud(width=800, height=800, background_color='white', max_words=250).generate(' '.join(df['tweet']))
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud_input_text)
plt.tight_layout(pad=0)
plt.axis('off')
plt.show()

"""Function for calculating and displaying the evaluation matrices"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc


def compute_performance(y_true, y_pred, clf):

  performance_dict = {} 
  precision = precision_score(y_true, y_pred)
  recall = recall_score(y_true, y_pred)
  F1_score = f1_score(y_true, y_pred, average='macro')
  print("#---------------Evalution Results---------------------#")
  
  print(classification_report(y_true,y_pred))
  class_labels = ['NOT', 'OFF']
  cm = confusion_matrix(y_true, y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
  disp.plot()
  plt.show()
   # Plotting ROC Curve
  fpr, tpr, thresholds = roc_curve(y_true, y_pred)
  roc_auc = auc(fpr, tpr)
  print("Precision: {:.4f}".format(precision))
  print("Recall: {:.4f}".format(recall))
  print("F1_Score", F1_score)
  print("ROC Score :", roc_auc )

  performance_dict['precision'] = precision  
  performance_dict['recall'] = recall  
  performance_dict['F1_score'] = F1_score 
  performance_dict['roc_score'] = roc_auc

  # Plotting the ROC curve
  plt.figure(figsize=(10, 6))
  plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rat')
  plt.ylabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.legend(loc='lower right')
  plt.show()

  return performance_dict

"""Function for preprocessing which will be reused for the preprocessing for all datasets.Following are the preprocessing done:
1. Handle the diacritics in the text
2. Removing the stop words 
3. Spelling correction
4. Removing the hashtag symbol
5. Removing the short words of length 1 and 2 characters
6. Removing the digits
7. Removing non-alphanumeric values
8. Removing URLs
9. Remove all strings starting with @
10. Removing additional stop words
"""

from textblob import TextBlob
nltk.download('punkt')
nltk.download("stopwords")

class pre_processing:
  def __init__(self,file):
    self.stop_words = set(stopwords.words('english'))
    self.file = file
    self.tokenizer = TweetTokenizer(preserve_case=True)


  def simplify(self, text):
    '''Function to handle the diacritics in the text'''
    import unicodedata
    try:
        text = unicode(text, 'utf-8')
    except NameError:
        pass
    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode("utf-8")
    return str(text)

  def remove_stopwords(self, text):                                             #Function to remove the stop words
    clean_text = [word for word in text if not word in self.stop_words]
    return clean_text  

  def spell_check(text):                                                        #Function to do spelling correction 
    txtblob = TextBlob(text)
    corrected_text = txtblob.correct()
    return corrected_text

  def remove_hashsymbols(self, text):                                           #Function to remove the # symbol
    pattern = re.compile(r'#')
    text = ' '.join(text)
    clean_text = re.sub(pattern,'',text)
    return self.tokenizer.tokenize(clean_text) 

  def rem_shortwords(self, text):
    lengths = [1,2]
    new_text = ' '.join(text)
    for word in text:
        text = [word for word in self.tokenizer.tokenize(new_text) if not len(word) in lengths]
        
    return new_text 

  def rem_digits(self, text):                                                   #Function to remove the digits from the list of strings
    no_digits = []
    for word in text:
        no_digits.append(re.sub(r'\d','',word))
    return ' '.join(no_digits)

  def rem_nonalpha(self, text):                                                 #Function to remove the non-alphanumeric characters from the text
    text = [word for word in text if word.isalpha()]
    return text

  def pre_process(self, file):
    data = pd.read_csv(file) 
    data.drop('id',axis=1,inplace=True)
    random = np.random.randint(0,len(data))
    data['tweet'] = data['tweet'].apply(self.simplify)

    
    data['tweet'].replace(r'@\w+','',regex=True,inplace=True)                   #remove all strings starting with @

                     
    data['tweet'].replace(r'http\S+','',regex=True,inplace=True)                  #remove all urls

    
    data['tweet'] = data['tweet'].apply(self.tokenizer.tokenize)                  #tokenize the tweets in the dataframe using TweetTokenizer
    
    stop_words = stopwords.words('english')

    
    additional_list = ['amp','rt','u',"can't",'ur']                               #add additional stop words to be removed from the text

    for words in additional_list:
        stop_words.append(words)

    data['tweet'] = data['tweet'].apply(self.remove_stopwords)
    
    data['tweet'] = data['tweet'].apply(self.remove_hashsymbols)                 #removing # symbols
  
    data['tweet'] = data['tweet'].apply(self.rem_shortwords)                     #Function to remove the short words of length one and two characters

    data['tweet'] = data['tweet'].apply(self.tokenizer.tokenize)

    data['label'] = data['label'].replace({'NOT': 0, 'OFF': 1}).values

    data['tweet'] = data['tweet'].apply(self.rem_digits)

    data['tweet'] = data['tweet'].apply(self.tokenizer.tokenize)

    data['tweet'] = data['tweet'].apply(self.rem_nonalpha)

    data['tweet'] = data['tweet'].apply(lambda x: ' '.join(x))                   #join the tokens back to form the string

    return data

"""Intalling joblib, which is used for saving the model to the drive"""

!pip install joblib

"""##Train Method 1

## BiLSTM

This cell is responsible for training the BiLSTM model, input data and validation data will be preprocessed. The preprocessed data will be vectorized and saved to drive. The preprocessed data is then tokenized using the Keras Tokenizer class, which turns the text into sequences of numbers. The word index used by the tokenizer is then recorded in a dictionary . To guarantee a constant input length for the LSTM layer, the sequences are then pad_sequences to a given length.And this will then passed to model for training and trained model will be saved to drive after validation using validation dataset.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Embedding
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Bidirectional
from keras.optimizers import Adam
import tensorflow as tf
import pickle
import joblib

def train_method1(train_file, val_file, model_dir):
   
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    pre_pro = pre_processing(train_file)                                        # Preprocessing the data
    train_df = pre_pro.pre_process(train_file)
    print("Train data shape :", train_df.shape)
    val_df = pre_pro.pre_process(val_file)
   
    X_train = train_df['tweet']
    y_train= train_df['label']
    X_val= val_df['tweet']
    y_val= val_df['label']

    vectorizer = CountVectorizer()                                              #Vectorizing the preprocesed data and saving it to drive
    vectorizer.fit_transform(X_train)
    vectorizer.fit_transform(X_val)

    with open(model_dir + '/vectorizer.sav', 'wb') as file:
      pickle.dump(vectorizer, file)

    max_features = 10000
    embedding_dim = 128
    max_len=500

    tokenizer=Tokenizer(num_words=max_features,oov_token='</OOV>')
    tokenizer.fit_on_texts(X_train.values)
    dic=tokenizer.word_index

    X_train_seq = tokenizer.texts_to_sequences(X_train.values)
    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)

    print("train data tensor:" ,X_train_pad.shape)

    X_val_seq = tokenizer.texts_to_sequences(X_val.values)
    X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)

    print("test data tensor:" ,X_val_pad.shape)
                                                                                #Model Architecture of BiLSTM
    model= Sequential()                                
    model.add(Embedding(max_features,embedding_dim,input_length=max_len))
    model.add(Bidirectional(LSTM(128, dropout=0.3)))
    model.add(Dense(128,activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(1,activation='sigmoid'))

    model.summary()
   
    METRICS = [tf.keras.metrics.BinaryAccuracy(),                                 # For validation during trainng
            tf.keras.metrics.Precision(name="precision"),
            tf.keras.metrics.Recall(name="recall")]

    model.compile(loss='binary_crossentropy',optimizer="adam",metrics=METRICS)

    epochs = 2
   
    history = model.fit(X_train_pad,y_train,validation_data=(X_val_pad,y_val), epochs=epochs)  # Fit the model using the train and test datasets.

    scores = model.evaluate(X_val_pad, y_val, return_dict=True)

    print(scores)
   
    train_loss = history.history['loss']                                         # Plot the training loss and validation loss
    val_loss = history.history['val_loss']
    epochs = range(1, len(train_loss) + 1)
    plt.plot(epochs, train_loss, label='Training Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training Loss vs Validation Loss')
    plt.legend()
    plt.show()

  
    print("Validation Dataset Accuracy")
    y_val_pred = model.predict(X_val_pad)
    for i in range(len(y_val_pred)):
      if(y_val_pred[i] > 0.5):
          y_val_pred[i] = 1
      else:
          y_val_pred[i] =0 
    compute = compute_performance(y_val, y_val_pred, model)

    joblib.dump(model, model_dir + '/model.sav')                                 # Save the trained model and tokenizer using pickle
    joblib.dump(tokenizer, model_dir + '/tokenizer.sav')
    return compute

"""Test method of BiLSTM model, where model will be loaded from the drive and predict the classification data and calls compute_perform function for evaluating the model."""

import pickle
import os
from sklearn import metrics
from tensorflow import keras
from keras import utils as keras_utils


def test_method1(test_file, model_file, output_dir):
    """
    Test a machine learning model on a test file.

    Args:
        test_file (str): File path of the test file.
        model_file (str): File path of the saved model file.
        output_dir (str): Directory path where the output files will be saved.

    Returns:
        dict: Dictionary containing computed performance metrics.
    """
    
    tokenizer = joblib.load(model_file +'/tokenizer.sav')


    # Load the trained model
    model = joblib.load(model_file  + '/model.sav')

    # Pre-process the test file
    pre_pro = pre_processing(test_file)
    test_df = pre_pro.pre_process(test_file) 

    X_test = test_df['tweet']
    y_test = test_df['label']
 
    # TEST
    X_test_seq = tokenizer.texts_to_sequences(X_test.values)
    X_test_pad = pad_sequences(X_test_seq, maxlen = 500)

    print("test data tensor:" ,X_test_pad.shape)

    scores = model.evaluate(X_test_pad, y_test, return_dict=True)

    print(scores)

    # Use the loaded model for testing on test data
    y_pred = model.predict(X_test_pad)

    for i in range(len(y_pred)):
      if(y_pred[i] > 0.5):
          y_pred[i] = 1
      else:
          y_pred[i] =0 

    #Saving the prdiction to the output directory
    mapping = {0: 'NOT', 1: 'OFF'}
    output_file = pd.read_csv(test_file)
    vfunc = np.vectorize(lambda x: mapping.get(x))
    y_pred_mapped = vfunc(y_pred)
    output_file['out_label'] = y_pred_mapped
    output_file.to_csv(output_dir, index=False)

    # Print accuracy score
    accuracy = metrics.accuracy_score(y_test, y_pred)
    print('Test Accuracy: {}'.format(accuracy))

    print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred)))
    compute = compute_performance(y_test, y_pred,model)

    return compute

"""##BiLSTM -  With 25 % datasize"""

train_method1(train_25, val_file, MODEL_1_25_DIRECTORY)

test_method1(test_file, MODEL_1_25_DIRECTORY, model_1_25_output_test_file)

"""##BiLSTM - With 50% data in training set"""

train_method1(train_50, val_file, MODEL_1_50_DIRECTORY)

test_method1(test_file, MODEL_1_50_DIRECTORY, model_1_50_output_test_file)

"""## BiLSTM - With 75% data in training set"""

train_method1(train_75, val_file, MODEL_1_75_DIRECTORY)

test_method1(test_file, MODEL_1_75_DIRECTORY, model_1_75_output_test_file)

"""## BiLSTM - With 100% dataset"""

train_method1(train_file, val_file, MODEL_1_100_DIRECTORY)

test_method1(test_file, MODEL_1_100_DIRECTORY, model_1_output_test_file)

"""## Method 1 End

## Training Method code 2

This method will be responsible for creating the dataset the format which tokenizer can take it as input. tokenizer, input data and label will be passed to this method and it will give a dictinary containing input id mask, attenstion mask and label.
"""

from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from sklearn.metrics import classification_report, confusion_matrix
import torch

class TextClassificationDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_token_type_ids=False,
            return_attention_mask=True,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": label,
        }

"""The training has been done in the methods , once the preprocessing is completed tokenizer will tokenize the data feed to model for training.Once the model is trained it will be evaluated using validation file and save the file to directory."""

def train_method2(train_file, val_file, model_dir):

  if not os.path.exists(model_dir):
      os.makedirs(model_dir)
  pre_pro = pre_processing(train_file)                                          #calling the function preprocessing

  train_df = pre_pro.pre_process(train_file)                                    # Preprocessing the data
  val_df = pre_pro.pre_process(val_file)
  
  X_train = train_df['tweet']
  y_train= train_df['label']
  
  X_val= val_df['tweet']
  y_val= val_df['label']

  vectorizer = CountVectorizer()
  vectorizer.fit_transform(X_train)
  vectorizer.fit_transform(X_val)

  with open(model_dir + '/vectorizer.sav', 'wb') as file:                       #Saving the vectorized file
    pickle.dump(vectorizer, file)

  tokenizer = ElectraTokenizer.from_pretrained("google/electra-base-discriminator")     # Load the ELECTRA tokenizer and model
  model = ElectraForSequenceClassification.from_pretrained("google/electra-base-discriminator")

  
  train_dataset = TextClassificationDataset(X_train, y_train, tokenizer, max_length=128)    # Prepare the dataset
  test_dataset = TextClassificationDataset(X_val, y_val, tokenizer, max_length=128)

  
  output_dir = './output'                                                       # Define the training arguments
  learning_rate = 5e-5                                                          #Setting learning rate
  training_args = TrainingArguments(
      output_dir=output_dir,
      per_device_train_batch_size=32,
      num_train_epochs=3,                                                       # setting the number of epochs
      save_steps=10_000,
      save_total_limit=2,
      logging_dir='./logs',  
      logging_steps=100,    
      learning_rate=learning_rate,  
  )

  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=train_dataset,
      eval_dataset=test_dataset,
      compute_metrics=None, 
  )

  
  trainer.train()                                                               # Train the model

  trainer.evaluate(test_dataset)                                                # Evaluate the model

  y_pred_probs = trainer.predict(test_dataset).predictions                       # Get predicted labels
  y_pred = torch.argmax(torch.tensor(y_pred_probs), axis=1).tolist()

  y_train_pred_probs = trainer.predict(train_dataset).predictions
  y_train_pred = torch.argmax(torch.tensor(y_train_pred_probs), axis=1).tolist()

  print("Validation dataset Evaluation:")

  compute_performance(y_val, y_pred, model)

  print('Validation: {}'.format(metrics.accuracy_score(y_val, y_pred)))

  tokenizer.save_pretrained(model_dir + '/tokenizer.sav')                       # save the model and tokenizer to drive, which will be used in test dataset
  model_file = model_dir + '/model.sav'

  torch.save({'tokenizer': tokenizer, 'model': model.state_dict()}, model_file)

  return tokenizer

"""## Testing Method 2 Code

ELECTRA Test method

In this function the model will be used to predict the values in test files and it will be evaluated using compute_performance function.
"""

def test_method2(test_file, model_file, output_dir):
  
  pre_pro = pre_processing(test_file)
  test_df = pre_pro.pre_process(test_file)
  X_test = test_df['tweet']
  y_test = test_df['label']

  checkpoint = torch.load(model_file + '/model.sav')

  tokenizer = ElectraTokenizer.from_pretrained(model_file + '/tokenizer.sav')
 
  model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', state_dict=checkpoint['model']) # Load the model

  model.eval()
  
  y_pred = []
  for example in X_test:                                                         # Loop through the test dataset and make predictions
      text = example
      inputs = tokenizer.encode_plus(text, return_tensors='pt', padding=True, truncation=True)
      with torch.no_grad():
          outputs = model(**inputs)
          logits = outputs.logits
          predicted_label = torch.argmax(logits, dim=1).item()
          y_pred.append(predicted_label)

  mapping = {0: 'NOT', 1: 'OFF'}
  output_file = pd.read_csv(test_file)
  vfunc = np.vectorize(lambda x: mapping.get(x))
  y_pred_mapped = vfunc(y_pred)
  output_file['out_label'] = y_pred_mapped
  output_file.to_csv(output_dir, index=False)

  print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred)))
  compute = compute_performance(y_test, y_pred, model)
  return compute

"""## ELECTRA - With 25% Data"""

train_method2(train_25, val_file, MODEL_2_25_DIRECTORY)

test_method2(test_file, MODEL_2_25_DIRECTORY, model_2_25_output_test_file)

"""## ELECTRA- With 50% dataset"""

train_method2(train_50, val_file, MODEL_2_50_DIRECTORY)

test_method2(test_file, MODEL_2_50_DIRECTORY, model_2_50_output_test_file)

"""## ELECTRA - With 75% Data"""

train_method2(train_75, val_file, MODEL_2_75_DIRECTORY)

test_method2(test_file, MODEL_2_75_DIRECTORY, model_2_75_output_test_file)

"""## ELECTRA- With 100% data"""

train_method2(train_file, val_file, MODEL_2_100_DIRECTORY)

test_method2(test_file, MODEL_2_100_DIRECTORY, model_2_100_output_test_file)

"""## Method 2 End

"""